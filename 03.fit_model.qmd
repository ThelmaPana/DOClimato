---
title: "Predict DOC from env data."
subtitle: "Fit a XGBoost model to predict DOC from environmental data and apply it to new data."
author: "Thelma Panaïotis"
format:
  html:
    toc: true
    embed-resources: true
editor: visual
execute:
  cache: false
  warning: false
---

## Set-up and load data

```{r set_up}
#|output: false
source("utils.R")
load("data/02.all_data.Rdata")
output_filename <- "data/03.doc_from_env_nested_cv.Rdata"
```

## Data preparation

### Variable roles

```{r roles}
# Response variable
resp_var <- c("doc_log", "doc") # keep both untransformed and transformed predictor
# Explanatory variables
#exp_vars <- df_all %>% select(nitrate:par) %>% colnames()
exp_vars <- df_all %>% 
  select(
    temperature,
    phosphate,
    silicate,
    alkalinity,
    dic,
    npp,
    oxygen
  ) %>%
  colnames()
# Metadata
meta_vars <- c("lon", "lat")
```

### Drop missing values

This is done after assigning roles to variables because the observations to be dropped depends on the set of explanatory variables.

```{r drop_missing}
# Dataset to fit the model
df <- df_all %>% 
  select(c(all_of(meta_vars), all_of(resp_var), all_of(exp_vars))) %>% 
  drop_na()

# New data to which apply predictions
df_env <- df_env %>% 
  select(c(all_of(meta_vars), all_of(exp_vars))) %>% 
  drop_na()
```

### Distribution of response variable

We want the response variable to be \~normally distributed. Let’s plot the distribution of doc_log.

```{r resp_dist}
ggplot(df) + geom_histogram(aes(x = doc_log), bins = 100)
```

Not too bad! Let’s proceed.

## Data split

To better assess the performance of our model and get a distribution of R² instead of a single value, we will use nested cross-validation.

Instead of splitting the data into train and test sets and get a single estimate of R² on the test set, we will use nested cross-validation so that we have several repeats of train/test splits and thus a distribution of R² values. For each fold, a second cross-validation is performed within the training set in order to tune the hyperparameters.

We use 10-fold cross-validation:

-   10% of data for testing

-   90% of data for training. This subset is used for nested cross-validation with:

    -   10% of data for validation

    -   90% of data for learning

Two different methods for data splitting are used:

-   stratified on deciles of the response variable (`doc_log`)

-   spatial cross-validation using blocks

```{r split}
# Transform dataframe to sf object for spatial CV.
df_sf <- st_as_sf(df, coords = c("lon", "lat"), crs = 4326)

set.seed(seed)
# Bind together folds of both nested_cv
folds <- bind_rows(
  # Stratified CV on deciles of response variable
  nested_cv(
    df,
    outside = vfold_cv(v = 10, strata = doc_log, breaks = 9),
    inside = vfold_cv(v = 10, strata = doc_log, breaks = 9)) %>%
    mutate(cv_type = "stratified"),
  # Spatial CV
  nested_cv(
    df_sf,
    outside = spatial_block_cv(v = 10),
    inside = spatial_block_cv(v = 10)) %>%
    mutate(cv_type = "spatial")
)
```

## Model definition

Let’s define a XGBoost regression model, with tunable hyperparameters:

-   `trees`: number of trees

-   `tree_depth`: maximum depth (i.e. number of splits) in a tree

-   `min_n`: minimum number of objects in a node to split further

-   `learn_rate`

```{r def_mod}
# Define a xgboost model with hyperparameters to tune
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = tune()
) %>%
  set_mode("regression") %>%
  set_engine("xgboost")
```

We also generate the formula from the explanatory variables. We also keep doc for now, but it will be removed from predictors when generating the recipe within the gridsearch.

```{r def_form}
# Generate formula from list of explanatory variables
xgb_form <- as.formula(paste("doc_log ~ ", paste(c("doc", exp_vars), collapse = " + "), sep = ""))
```

Finally, let’s define the grid for the gridsearch (only one grid is used for all folds).

```{r def_grid}
# Define one grid for all folds
set.seed(seed)
xgb_grid <- grid_latin_hypercube(
  trees(),
  learn_rate(),
  tree_depth(),
  min_n(),
  size = 10
)
```

## Models fitting

Let’s loop on cv folds. For each fold, a gridsearch is performed using nested CV on training data, and performance are assessed on the test data. This is run in parallel on `r n_cores` cores.

```{r gridsearch}
res <- mclapply(1:nrow(folds), function(i){
  ## Get fold
  x <- folds[i,]

  ## Train and test sets
  df_train <- analysis(x$splits[[1]]) %>% as_tibble()
  df_test <- assessment(x$splits[[1]]) %>% as_tibble()

  ## Recipe
  xgb_rec <- recipe(xgb_form, data = df_train) %>%
    update_role(doc, new_role = "untransformed outcome")

  ## Workflow
  xgb_wflow <- workflow() %>%
    add_recipe(xgb_rec) %>%
    add_model(xgb_spec)

  ## Gridsearch
  set.seed(seed)
  xgb_res <- tune_grid(
    xgb_wflow,
    resamples = x$inner_resamples[[1]],
    grid = xgb_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_pred = TRUE)
  )
  best_params <- select_best(xgb_res)

  ## Final fit
  final_xgb <- finalize_workflow(
    xgb_wflow,
    best_params
  )
  final_res <- fit(final_xgb, df_train)

  ## Prediction on outer folds
  preds <- predict(final_res, new_data = df_test) %>%
    bind_cols(df_test %>% select(doc_log))

  ## Model explainer
  # Select only predictors
  vip_train <- xgb_rec %>% prep() %>% bake(new_data = NULL, all_predictors())

  # Explainer
  xgb_explain <- explain_tidymodels(
      model = extract_fit_parsnip(final_res),
      data = vip_train,
      y = df_train %>%  pull(doc_log),
      verbose = FALSE
    )

  # Variable importance
  full_vip <- model_parts(xgb_explain) %>%
    bind_rows() %>%
    filter(variable != "_baseline_")

  # CP profiles for all variables
  #selected_points <- ingredients::select_sample(df_train, n = 100, seed = seed)
  #cp_profiles <- ingredients::ceteris_paribus(xgb_explain, selected_points) %>% as_tibble()
  # CP profiles
  cp_profiles <- lapply(exp_vars, function(my_var){
    model_profile(explainer = xgb_explain, variables = my_var)$cp_profiles %>% as_tibble()
  }) %>%
    bind_rows()
  
  ## Predict new data
  new_preds <- augment(final_res, new_data = df_env %>% mutate(doc = NA)) %>% 
    select(-doc) %>% 
    rename(pred_doc_log = .pred) %>% 
    relocate(pred_doc_log, .after = lat)
  
  ## Return results
  return(tibble(
      preds = list(preds),
      importance = list(full_vip),
      cp_profiles = list(cp_profiles),
      fold = x$id,
      cv_type = x$cv_type,
      new_preds = list(new_preds)
    ))
}, mc.cores = 10) %>%
  bind_rows()
```

## Save results

```{r save}
#save(res, file = output_filename)
```

```{r}



titi <- cp_profiles %>% 
  filter(cv_type == "stratified" & fold == "Fold02") %>% 
  pull(alkalinity) %>% 
  unique() %>% sort()

cp_profiles %>%
  # filter for given fold and variable
  filter(cv_type == "stratified" & `_vname_` == "temperature") %>%
  # center each cp profiles across fold, variable and ids
  group_by(fold, `_vname_`, `_ids_`) %>%
  mutate(yhat_cent = `_yhat_` - mean(`_yhat_`)) %>% # center cp profiles
  ungroup() %>%
  # compute mean and sd of centered cp profiles for each fold and value of the variable of interest
  group_by(fold, temperature) %>%
  summarise(
    yhat_loc = mean(`_yhat_`), # compute mean of profiles
    yhat_spr = sd(yhat_cent), # compute sd of cp profiles
    .groups = "keep"
  ) %>%
  ungroup() %>%
  # sort by increasing value of variable of interest
  arrange(temperature) %>%
  # rename
  setNames(c("fold", "x", "yhat_loc", "yhat_spr")) %>%
  ggplot() +
  geom_ribbon(aes(x = x, ymin = yhat_loc - yhat_spr, ymax = yhat_loc + yhat_spr), fill = "gray80", alpha = 0.5) +
  geom_path(aes(x = x, y = yhat_loc), color = "midnightblue", linewidth = 1.2) +
  labs(x = x$variable, title = paste("PDP for", x$variable), subtitle = paste(x$cv_type, "cross-validation")) +
  facet_wrap(~fold, ncol = 2)

cp_profiles %>%
  # filter for given fold and variable
  filter(cv_type == "stratified" & `_vname_` == "temperature") %>%
  #filter(fold == "Fold01") %>% 
  # center each cp profiles across fold, variable and ids
  group_by(fold, `_vname_`, `_ids_`) %>%
  mutate(yhat_cent = `_yhat_` - mean(`_yhat_`)) %>% # center cp profiles
  ungroup() %>%
  # compute mean and sd of centered cp profiles for each fold and value of the variable of interest
  group_by(fold, temperature) %>%
  summarise(
    yhat_loc = mean(`_yhat_`), # compute mean of profiles
    yhat_spr = sd(yhat_cent), # compute sd of cp profiles
    .groups = "keep"
  ) %>%
  ungroup() %>%
  # sort by increasing value of variable of interest
  arrange(temperature) %>%
  # rename
  setNames(c("fold", "x", "yhat_loc", "yhat_spr")) %>%
  ggplot() +
  #geom_ribbon(aes(x = x, ymin = yhat_loc - yhat_spr, ymax = yhat_loc + yhat_spr), fill = "gray80", alpha = 0.5) +
  geom_path(aes(x = x, y = yhat_loc), color = "midnightblue", linewidth = 1.2) +
  geom_rug(aes(x = x)) +
  #labs(x = x$variable, title = paste("PDP for", x$variable), subtitle = paste(x$cv_type, "cross-validation")) +
  facet_wrap(~fold, ncol = 2)
  
```

```{r}
# Unnest cp_profiles
cp_profiles <- res %>% select(cv_type, fold, cp_profiles) %>% unnest(cp_profiles)

## Do it separately for each variable and each cv_type
# Within each fold, compute the mean
n_folds <- 10

mean_pdp <- lapply(1:nrow(vars_pdp), function(r){
  
  # Get variable and cvtype
  var_name <- vars_pdp[r,]$variable
  cv_type_name <- vars_pdp[r,]$cv_type
  
  # Get corresponding CP profiles
  # Compute mean and spread for each fold
  d_pdp <- cp_profiles %>% 
    filter(cv_type == cv_type_name & `_vname_` == var_name) %>% 
    select(cv_type, fold, `_yhat_`, `_vname_`, `_ids_`, all_of(var_name)) %>% 
    arrange(`_ids_`, across(all_of(var_name))) %>% 
    # center each cp profiles across fold, variable and ids
    group_by(cv_type, fold, `_vname_`, `_ids_`) %>%
    mutate(yhat_cent = `_yhat_` - mean(`_yhat_`)) %>% # center cp profiles
    ungroup() %>%
    # compute mean and sd of centered cp profiles for each fold and value of the variable of interest
    group_by(cv_type, fold, across(all_of(var_name))) %>%
    summarise(
      yhat_loc = mean(`_yhat_`), # compute mean of profiles
      yhat_spr = sd(yhat_cent), # compute sd of cp profiles
      .groups = "keep"
    ) %>%
    ungroup() %>% 
    setNames(c("cv_type", "fold", "x", "yhat_loc", "yhat_spr"))
  
  # Regularise across folds: need a common x distribution, and interpolate y on this new x
  new_x <- quantile(d_pdp$x, probs = seq(0, 1, 0.01), names = FALSE)
  # x is different within each fold, so interpolation is performed on each fold
  folds <- unique(d_pdp$fold) %>% sort()
  int_pdp <- lapply(1:n_folds, function(i){
    # Get data corresponding to this fold
    fold_name <- folds[i]
    this_fold <- d_pdp %>% filter(fold == fold_name)
    #cv_type_name <- bar %>% pull(cv_type) %>% unique()
    
    # Extract original x values
    x <- this_fold$x
    # Extract values to interpolate (yhat_loc and yhat_spr)
    yhat_loc <- this_fold$yhat_loc
    yhat_spr <- this_fold$yhat_spr
    # Interpolate yhat_loc and yhat_spr on new x values
    int <- tibble(
      x = new_x,
      yhat_loc = castr::interpolate(x = x, y = yhat_loc, xout = new_x),
      yhat_spr = castr::interpolate(x = x, y = yhat_spr, xout = new_x),
    ) %>% 
      mutate(
        cv_type = cv_type_name,
        fold = fold_name,
        var_name = var_name,
        .before = x
        )
    # Return the result
    return(int)
    
  }) %>% 
    bind_rows()
  
  # Across fold, compute the weighted mean, using 1/var as weights
  mean_pdp <- int_pdp %>% 
    group_by(cv_type, var_name, x) %>% 
    summarise(
      yhat_loc = wtd.mean(yhat_loc, weights = 1/(yhat_spr)^2),
      yhat_spr = wtd.mean(yhat_spr, weights = 1/(yhat_spr)^2),
      .groups = "drop"
    ) %>% 
    arrange(x)
  
  # Return the result
  return(mean_pdp)
}) %>% 
  bind_rows()



ggplot(mean_pdp) + 
  geom_path(aes(x = x, y = yhat_loc, colour = cv_type)) +
  geom_ribbon(aes(x = x, ymin = yhat_loc - yhat_spr, ymax = yhat_loc + yhat_spr, fill = cv_type), alpha = 0.2) +
  facet_wrap(~var_name, scales = "free_x")
  
```
